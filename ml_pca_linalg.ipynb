{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materials\n",
    "\n",
    "  - [Recommended Youtube playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab). Watch at least chapters 3, 4, 13 and 14, but you can watch all of them if you find them to your interest.\n",
    "  - [Video explaining PCA method](https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer).\n",
    "  - [Article explaining PCA method](https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Overview\n",
    "\n",
    "A dataset is characterized (among other factors) by:\n",
    "\n",
    "  - Size: Number of data points of the dataset. Often also called \"length\".\n",
    "  - Dimensionality: Number of features (variables) per data point.\n",
    "\n",
    "In Machine Learning, the \"curse of dimensionality\" refers to the phenomenon that as the dimensionality of a dataset grows, the amount of information encoded in the features gets smaller - features start becoming irrelevant. This is dangerous because it leads to:\n",
    "\n",
    "  - Generalization issues: Models may focus on irrelevant variables that may not be useful when it comes to out-of-distribution data coming from real life in a production setting.\n",
    "  - Loss of explainability: Having an excessive amount of features makes it difficult to understand the decision process behind the models.\n",
    "\n",
    "Dimensionality reduction techniques are those which aim to reduce the dimensionality of a dataset by identifying irrelevant variables and removing them, or by combining the dataset variables into new variables with way more encoded information. One of these techniques is PCA (Principal Component Analysis)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "  - Download the \"Wine Quality\" dataset from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/datasets).\n",
    "  - Using `pandas`, read and view the csv file contianing data for red wine. Take a look at the dataset size and dimensionality.\n",
    "  - For each feature, compute the minimum, mean and maximum values, and inspect them.\n",
    "\n",
    "**Useful resources**\n",
    "\n",
    "  - [pandas.read](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "  - [pandas.DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "  - Convert the data from the pandas dataframe into a numpy array.\n",
    "  - Standardize the numpy array. To do this:\n",
    "    - Compute the mean and standard deviation of each feature.\n",
    "    - Substract the mean to all rows, then divide by the standard deviation.\n",
    "\n",
    "Remember to remove the target variable. You can do this by removing the `quality` column from the dataframe beforehand, or by dropping the last column of the resulting numpy array.\n",
    "\n",
    "\n",
    "**Useful resources**\n",
    "\n",
    "  - [pandas.DataFrame.to_numpy](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html)\n",
    "  Construct a numpy array of shape \n",
    "  - [numpy.mean](https://numpy.org/devdocs/reference/generated/numpy.mean.html)\n",
    "  - [numpy.std](https://numpy.org/devdocs/reference/generated/numpy.std.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "\n",
    "  - Compute the covariance matrix of your standardized array.\n",
    "\n",
    "Assume your dataset has size $N$ (number of data points) and dimensionality $D$ (number of variables). One can understand each of the $D$ columns of your dataset as a vector of samples taken from a random variable. With that context in mind, it is possible for us to compute the covariance matrix between all these random variables.\n",
    "\n",
    "Given a set of random variables, their covariance matrix is a matrix which contains the [covariance](https://en.wikipedia.org/wiki/Covariance) between random variables $i$ and $j$ in position $(i, j)$. In our case, because the dimensionality of the dataset is $D$, we have $D$ random variables, so the matrix will have shape $D \\times D$.\n",
    "\n",
    "Formula for the covariance between two columns $x^j$ and $x^k$ ($j$ and $k$ are indices, not exponents):\n",
    "\n",
    "$$\n",
    "Cov_{j, k} = \\frac{1}{N} \\sum_{i = 1}^{N} (x^j_i - \\mu^j) (x^k_i - \\mu^k)\n",
    "$$\n",
    "\n",
    "Remember: Because you standardized your dataset before computing the covariance matrix, the mean of all your standardized variables is 0. This means you can take shortcuts rather than applying the usual covariance formula.\n",
    "\n",
    "Hint: You can use the dot product (matricial product).\n",
    "\n",
    "  - Visualize the covariance matrix of your standardized array. Use `plt.imshow` and explore its options. Show the different labels of each variable. Which variables are the most correlated between each other? What does this mean in practice?\n",
    "\n",
    "**Useful resources**\n",
    "\n",
    "  - [numpy.dot](https://numpy.org/devdocs/reference/generated/numpy.dot.html), or alternatively, the `@` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "\n",
    "  - Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "The covariance operator is symmetric, therefore, the covariance matrix is symmetric. If a matrix is symmetric, all its eigenvalues are positive values. This means that you will ALWAYS be able to diagonalize this matrix (some matrices are not diagonalizable because some of its eigenvalues lie on the complex plane, rather than the real line).\n",
    "\n",
    "  - Order the eigenvectors by their eigenvalue (descending), and display them. Attempt to interpret them.\n",
    "\n",
    "The resulting eigenvectors are new variables (created by linearly combining all other variables) that are guaranteed to be independent from each other. Due to the nature of these new variables, it may be difficult to understand what they exactly mean. \n",
    "\n",
    "The eigenvalue associated to each eigenvector signifies the amount of variance associated to the variable associated to that eigenvector (the higher this number, the most relevant this new variable is).\n",
    "\n",
    "  - Plot a curve showing the percentage of variance associated to the new variables.\n",
    "\n",
    "Given the ordered (descending) eigenvalues $\\lambda_1, \\dots, \\lambda_D$ obtained by this method, we can measure the **percentage of variance** associated to the first $d$ variables with the following formula:\n",
    "\n",
    "$$\n",
    "\\textrm{PV}_d = \\frac{\\sum_{i=1}^{d} \\lambda_i}{\\sum_{i=1}^{D} \\lambda_i}\n",
    "$$\n",
    "\n",
    "**Useful resources**\n",
    "\n",
    "  - [numpy.linalg](https://numpy.org/doc/2.1/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig) - numpy utilities for linear algebra. The `numpy.linalg.eig` method will compute eigenvalues and eigenvectors for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Activity\n",
    "\n",
    "The `sklearn` Python module provides implementations for many dimensionality reduction algoriths, including PCA. In practice, we do not implement PCA ourselves, but use what `sklearn` offers us.\n",
    "\n",
    "Follow these steps for a complete analysis on how PCA affects the performance of a regressor. Make sure to carefully analyze the input and the output of the code you execute, and utilize the interactive capabilities of the jupyter notebook by printing stuff, viewing the variables... **Do not treat this as writing a Python script**.\n",
    "\n",
    "  1. Extract, from the original dataset dataframe, two numpy arrays `x` and `y` with the data and labels.\n",
    "    - The `y` array is just the `quality` column, which should be of shape $1599$.\n",
    "    - The `x` array is everything else except for the `quality` column, which should be of shape $1599 \\times 11$ in our particular case.\n",
    "  2. The `sklearn.model_selection.train_test_split` function will automatically create train and test splits for you with just a oneliner. Using it, create arrays `x_train, x_test, y_train, y_test` (train and test splits). Use a ratio of 20% for the test split.\n",
    "  3. Create a `sklearn.decomposition.PCA` object and fit it using the `x_train` array. After this, the PCA object will be ready to transform any samples of data into the new PCA variables. Use the PCA object to transform the `x_train, x_test` arrays into `x_train_pca, x_test_pca` arrays.\n",
    "  4. After fitting the PCA object, it will store the variance of each new variable ordered (descending) in the `components_` attribute. Plot a curve showing the percentage of variance associated to the new variables.\n",
    "  5. Choose any regressor provided by `sklearn`, for example a Support Vector Regressor `sklearn.svm.SVR`.\n",
    "    - Train it on `x_train_pca, y_train`.\n",
    "    - Evaluate it on `x_train_pca` to obtain `y_train_pred`. Compute the MAE and MSE between `y_train` and `y_train_pred`.\n",
    "    - Evaluate it on `x_test_pca` to obtain `y_test_pred`. Compute the MAE and MSE between `y_test` and `y_test_pred`.\n",
    "    - Display the obtained train and test MAE and MSE values. Interpret them.\n",
    "  6. Repeat step 5, but now using the first $1, 2, 3, \\dots, 11$ columns with the most variance for training and evaluating your regressor. Plot the following curves:\n",
    "    - Train MAE wrt. number of columns.\n",
    "    - Train MSE wrt. number of columns.\n",
    "    - Test MAE wrt. number of columns.\n",
    "    - Test MSE wrt. number of columns.\n",
    "  7. Observe the plots. After how many columns the performance of our regressor stops increasing? How many columns did we originally have? What are your conslusions from this analysis?\n",
    "\n",
    "**Useful resources**\n",
    "\n",
    "  - [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "  - [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "  - [sklearn.svm.SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "  - Reminder: MAE means [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error).\n",
    "  - Reminder: MSE means [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2cb538239ff5792ab8489b2aec6ee5c264edcebfa0e19cb03b1021ec9f17cac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
